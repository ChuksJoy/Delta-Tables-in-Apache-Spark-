# Use Delta Tables in Apache Spark

This project showcases my hands-on experience building a robust **data lakehouse** using **Apache Spark** and **Delta Lake** within **Microsoft Fabric**. I explored the full lifecycle of Delta Tables â€“ from data ingestion to real-time streaming analytics â€“ all while leveraging the open-source Delta Lake format for reliability, scalability, and performance.

---

### What I Did

As part of my exploration of modern data architecture and lakehouse design, I completed the following steps:

- ğŸ—ï¸ **Created a Microsoft Fabric workspace** to serve as my development environment.
- ğŸ’¾ **Built a Lakehouse** and **uploaded raw datasets**, simulating real-world data ingestion.
- ğŸ§® **Explored the data using PySpark DataFrames**, applying transformations and data wrangling operations.
- ğŸ§Š **Created Delta Tables** from the ingested data to enable ACID-compliant, versioned storage.
- ğŸ“ **Used SQL to define and query Delta Tables**, allowing for flexible analytics and reporting.
- ğŸ” **Explored Delta table versioning** to understand time travel, rollback, and incremental updates.
- ğŸ“Š **Ran advanced SQL queries** to analyze trends and derive business insights.
- ğŸŒŠ **Simulated real-time data streaming** with Delta tables, demonstrating support for unified batch and streaming pipelines.

---

### Tools & Technologies

- **Apache Spark (PySpark)**
- **Delta Lake**
- **Microsoft Fabric (Lakehouse)**
- **SQL & Python**
- **Streaming & Batch Data Processing**

---

### Key Takeaways

- âœ”ï¸ Built a scalable data lakehouse architecture from scratch.
- âœ”ï¸ Hands-on experience with Delta Lake features: ACID transactions, schema enforcement, time travel, and versioning.
- âœ”ï¸ Demonstrated the power of unifying batch and streaming data pipelines within a single framework.
- âœ”ï¸ Strengthened my skills in SQL analytics, data engineering, and cloud-native data infrastructure.

---

### Why This Matters

This project reinforced my ability to **design, implement, and manage data solutions** that are performant, reliable, and ready for production-scale analytics. It aligns with industry best practices for **modern data engineering** and showcases my capacity to work with **enterprise-grade tools** in the cloud.

---

PS: This is part of my AI skill Fest Project
