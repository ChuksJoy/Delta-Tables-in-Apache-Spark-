# Use Delta Tables in Apache Spark

This project showcases my hands-on experience building a robust **data lakehouse** using **Apache Spark** and **Delta Lake** within **Microsoft Fabric**. I explored the full lifecycle of Delta Tables – from data ingestion to real-time streaming analytics – all while leveraging the open-source Delta Lake format for reliability, scalability, and performance.

---

### What I Did

As part of my exploration of modern data architecture and lakehouse design, I completed the following steps:

- 🏗️ **Created a Microsoft Fabric workspace** to serve as my development environment.
- 💾 **Built a Lakehouse** and **uploaded raw datasets**, simulating real-world data ingestion.
- 🧮 **Explored the data using PySpark DataFrames**, applying transformations and data wrangling operations.
- 🧊 **Created Delta Tables** from the ingested data to enable ACID-compliant, versioned storage.
- 📝 **Used SQL to define and query Delta Tables**, allowing for flexible analytics and reporting.
- 🔁 **Explored Delta table versioning** to understand time travel, rollback, and incremental updates.
- 📊 **Ran advanced SQL queries** to analyze trends and derive business insights.
- 🌊 **Simulated real-time data streaming** with Delta tables, demonstrating support for unified batch and streaming pipelines.

---

### Tools & Technologies

- **Apache Spark (PySpark)**
- **Delta Lake**
- **Microsoft Fabric (Lakehouse)**
- **SQL & Python**
- **Streaming & Batch Data Processing**

---

### Key Takeaways

- ✔️ Built a scalable data lakehouse architecture from scratch.
- ✔️ Hands-on experience with Delta Lake features: ACID transactions, schema enforcement, time travel, and versioning.
- ✔️ Demonstrated the power of unifying batch and streaming data pipelines within a single framework.
- ✔️ Strengthened my skills in SQL analytics, data engineering, and cloud-native data infrastructure.

---

### Why This Matters

This project reinforced my ability to **design, implement, and manage data solutions** that are performant, reliable, and ready for production-scale analytics. It aligns with industry best practices for **modern data engineering** and showcases my capacity to work with **enterprise-grade tools** in the cloud.

---

PS: This is part of my AI skill Fest Project
